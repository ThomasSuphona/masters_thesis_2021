{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From video to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from PIL import Image\n",
    "import PIL.ImageOps as pimo\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert video to image sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_frames(input_loc, output_loc):\n",
    "    \"\"\"Function to extract frames from input video file\n",
    "    and save them as separate frames in an output directory.\n",
    "    Args:\n",
    "        input_loc: Input video file.\n",
    "        output_loc: Output directory to save the frames.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.mkdir(output_loc)\n",
    "    except OSError:\n",
    "        pass\n",
    "    # Log the time\n",
    "    time_start = time.time()\n",
    "    # Start capturing the feed\n",
    "    cap = cv2.VideoCapture(input_loc)\n",
    "    # Find the number of frames\n",
    "    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))-1\n",
    "    print (\"Number of frames: \", video_length)\n",
    "    count = 0\n",
    "    print (\"Converting video..\\n\")\n",
    "    # Start converting the video\n",
    "    while cap.isOpened():\n",
    "        # Extract the frame\n",
    "        ret, frame = cap.read()\n",
    "        # Write the results back to output location.\n",
    "        cv2.imwrite(output_loc + \"/%#05d.jpg\" % (count+1), frame)\n",
    "        count = count + 1\n",
    "        # If there are no more frames left\n",
    "        if (count > (video_length-1)):\n",
    "            # Log the time again\n",
    "            time_end = time.time()\n",
    "            # Release the feed\n",
    "            cap.release()\n",
    "            # Print stats\n",
    "            print (\"Done extracting frames.\\n%d frames extracted\" % count)\n",
    "            print (\"It took %d seconds for conversion.\" % (time_end-time_start))\n",
    "            break\n",
    "\n",
    "input_loc = './input_videos/20B0P0M.mp4'\n",
    "output_loc = './image_sequence/'\n",
    "video_to_frames(input_loc, output_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undistort image sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM=(1920, 1080)\n",
    "K=np.array([[995.9674795793435, 0.0, 980.9711898019134], [0.0, 997.8580870545059, 525.876314273917], [0.0, 0.0, 1.0]])\n",
    "D=np.array([[0.039014117003568535], [-0.13564797917630375], [0.19245112256088762], [-0.0946605817791271]])\n",
    "\n",
    "def undistort(input_loc, output_loc, balance=0.0, dim2=None, dim3=None):\n",
    "    \n",
    "    \n",
    "    path = glob.glob(input_loc)\n",
    "  \n",
    "    for frame, img in enumerate(path):\n",
    "        img = cv2.imread(img)\n",
    "        \n",
    "        #Undistort part\n",
    "        dim1 = img.shape[:2][::-1]  #dim1 is the dimension of input image to un-distort\n",
    "        assert dim1[0]/dim1[1] == DIM[0]/DIM[1], \"Image to undistort needs to have same aspect ratio as the ones used in calibration\"\n",
    "        if not dim2:\n",
    "            dim2 = dim1\n",
    "        if not dim3:\n",
    "            dim3 = dim1\n",
    "        scaled_K = K * dim1[0] / DIM[0]  # The values of K is to scale with image dimension.\n",
    "        scaled_K[2][2] = 1.0  # Except that K[2][2] is always 1.0\n",
    "        # This is how scaled_K, dim2 and balance are used to determine the final K used to un-distort image. OpenCV document failed to make this clear!\n",
    "        new_K = cv2.fisheye.estimateNewCameraMatrixForUndistortRectify(scaled_K, D, dim2, np.eye(3), balance=balance)\n",
    "        map1, map2 = cv2.fisheye.initUndistortRectifyMap(scaled_K, D, np.eye(3), new_K, dim3, cv2.CV_16SC2)\n",
    "        undistorted_img = cv2.remap(img, map1, map2, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n",
    "\n",
    "        #Write to the output path\n",
    "        filename = path[frame].rsplit('\\\\')[-1]\n",
    "        output_path = output_loc+filename\n",
    "\n",
    "        #cv2.imshow(\"undistorted\", undistorted_img)\n",
    "        cv2.imwrite(output_path,undistorted_img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "#if __name__ == '__main__':\n",
    "#    for p in sys.argv[1:]:\n",
    "#        undistort(p)\n",
    "\n",
    "\n",
    "input_loc = './input_test/*.jpg'\n",
    "output_loc = './undistort_test/'\n",
    "\n",
    "#input_loc = './image_sequence/*.jpg'\n",
    "#output_loc = './undistort_sequence/'\n",
    "\n",
    "balance = 0\n",
    "undistort(input_loc, output_loc, balance=balance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invert and grayscale the images and crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(input_loc, output_loc):\n",
    "    images = glob.glob(input_loc)\n",
    "    \n",
    "    for image in images:\n",
    "        filename = image.rsplit('\\\\')[-1]\n",
    "        output_path = output_loc + filename\n",
    "\n",
    "        image = Image.open(image)\n",
    "        image = pimo.invert(image)\n",
    "        image = pimo.grayscale(image)\n",
    "        image = pimo.crop(image, border=(435, 85, 440, 100))\n",
    "        image.show()\n",
    "        image.save(output_path)\n",
    " \n",
    "\n",
    "input_loc = './undistort_test/*.jpg'\n",
    "output_loc = './preprocess_test/'\n",
    "\n",
    "#input_loc = './undistort_sequence/*.jpg'\n",
    "#output_loc = './20B0P0M/'\n",
    "\n",
    "filter(input_loc, output_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort_and_filter(input_loc, output_loc, balance=0.0, dim2=None, dim3=None):\n",
    "\n",
    "        #Parameters to undistort from fisheye\n",
    "        DIM = (1920, 1080)\n",
    "        K = np.array([[995.9674795793435, 0.0, 980.9711898019134],\n",
    "                      [0.0, 997.8580870545059, 525.876314273917],\n",
    "                      [0.0, 0.0, 1.0]])\n",
    "\n",
    "        D = np.array([[0.039014117003568535],\n",
    "                      [-0.13564797917630375],\n",
    "                      [0.19245112256088762],\n",
    "                      [-0.0946605817791271]])\n",
    "\n",
    "\n",
    "        img = cv2.imread(input_loc)\n",
    "        \n",
    "        # Undistort part\n",
    "        dim1 = img.shape[:2][::-1]  # dim1 is the dimension of input image to un-distort\n",
    "        assert dim1[0] / dim1[1] == DIM[0] / DIM[1], \"Image to undistort needs to have same aspect ratio as the ones used in calibration\"\n",
    "        if not dim2:\n",
    "            dim2 = dim1\n",
    "        if not dim3:\n",
    "            dim3 = dim1\n",
    "        scaled_K = K * dim1[0] / DIM[0]  # The values of K is to scale with image dimension.\n",
    "        scaled_K[2][2] = 1.0  # Except that K[2][2] is always 1.0\n",
    "        # This is how scaled_K, dim2 and balance are used to determine the final K used to\n",
    "        # un-distort image. OpenCV document failed to make this clear!\n",
    "        new_K = cv2.fisheye.estimateNewCameraMatrixForUndistortRectify(scaled_K, D, dim2, np.eye(3),\n",
    "                                                                       balance=balance)\n",
    "        map1, map2 = cv2.fisheye.initUndistortRectifyMap(scaled_K, D, np.eye(3), new_K, dim3, cv2.CV_16SC2)\n",
    "        undistorted_img = cv2.remap(img, map1, map2, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n",
    "        \n",
    "\n",
    "        #Do invert and greyscale\n",
    "        invert_img = (255-undistorted_img)\n",
    "        grey_img = cv2.cvtColor(invert_img, cv2.COLOR_BGR2GRAY)\n",
    "        cropped_img = grey_img[85:978, 435:1478]#435, 85, 440, 100\n",
    "        #cv2.imshow(\"cropped\", grey_img)\n",
    "        cv2.imwrite(output_loc + \"/cropped.jpg\", cropped_img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "input_loc = './input_test/test.jpg'\n",
    "output_loc = './undistort_test/'\n",
    "undistort_and_filter(input_loc, output_loc, test=False, balance=0.0, dim2=None, dim3=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
